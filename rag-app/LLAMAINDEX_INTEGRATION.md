# LlamaIndex ä¸ xb é›†æˆæŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—å±•ç¤ºå¦‚ä½•å°† **xbï¼ˆGoï¼‰** ä¸ **LlamaIndexï¼ˆPythonï¼‰** é›†æˆï¼Œæ„å»ºé«˜æ€§èƒ½çš„ RAG åº”ç”¨ã€‚

**æ¶æ„ä¼˜åŠ¿**ï¼š
- âœ… **Go åç«¯**ï¼šé«˜æ€§èƒ½å‘é‡æ£€ç´¢ï¼ˆxb + Qdrant/PostgreSQLï¼‰
- âœ… **Python å‰ç«¯**ï¼šä¸°å¯Œçš„ LLM ç”Ÿæ€ï¼ˆLlamaIndexï¼‰
- âœ… **è§£è€¦è®¾è®¡**ï¼šå„è‡ªå‘æŒ¥æ‰€é•¿

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ–¹æ¡ˆ Aï¼šxb ä½œä¸º HTTP API æœåŠ¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python/LlamaIndexï¼ˆAI å±‚ï¼‰              â”‚
â”‚  - LLM è°ƒç”¨                              â”‚
â”‚  - Prompt å·¥ç¨‹                           â”‚
â”‚  - ç»“æœåå¤„ç†                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Go/xb API æœåŠ¡ï¼ˆæ£€ç´¢å±‚ï¼‰                â”‚
â”‚  - å‘é‡æ£€ç´¢ï¼ˆxbï¼‰                        â”‚
â”‚  - æ•°æ®åº“æŸ¥è¯¢ï¼ˆPostgreSQL/Qdrantï¼‰       â”‚
â”‚  - é«˜æ€§èƒ½å¤„ç†                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ å®ç°æ­¥éª¤

### Step 1: åˆ›å»º xb API æœåŠ¡ï¼ˆGoï¼‰

```go
// main.go
package main

import (
    "encoding/json"
    "net/http"
    "github.com/gin-gonic/gin"
    "github.com/fndome/xb"
)

type SearchRequest struct {
    Query     string                 `json:"query"`
    Embedding []float32              `json:"embedding"`
    TopK      int                    `json:"top_k"`
    Filters   map[string]interface{} `json:"filters,omitempty"`
}

type SearchResponse struct {
    Documents []Document `json:"documents"`
    Took      int64      `json:"took_ms"`
}

func main() {
    r := gin.Default()
    
    // â­ å‘é‡æ£€ç´¢ API
    r.POST("/api/vector/search", func(c *gin.Context) {
        var req SearchRequest
        if err := c.ShouldBindJSON(&req); err != nil {
            c.JSON(400, gin.H{"error": err.Error()})
            return
        }
        
        // ä½¿ç”¨ xb æ„å»ºæŸ¥è¯¢
        queryVector := xb.Vector(req.Embedding)
        
        builder := xb.Of("document_chunks").
            Custom(xb.QdrantBalanced()).
            VectorSearch("embedding", queryVector, req.TopK)
        
        // åº”ç”¨è¿‡æ»¤å™¨
        if docType, ok := req.Filters["doc_type"].(string); ok && docType != "" {
            builder = builder.Eq("doc_type", docType)
        }
        
        if lang, ok := req.Filters["language"].(string); ok && lang != "" {
            builder = builder.Eq("language", lang)
        }
        
        built := builder.Build()
        
        // ç”Ÿæˆ Qdrant JSON
        qdrantJSON, err := built.JsonOfSelect()
        if err != nil {
            c.JSON(500, gin.H{"error": err.Error()})
            return
        }
        
        // è°ƒç”¨ Qdrant
        results := qdrantClient.Search(qdrantJSON)
        
        c.JSON(200, SearchResponse{
            Documents: results,
            Took:      measureTime(),
        })
    })
    
    r.Run(":8080")
}
```

---

### Step 2: LlamaIndex é›†æˆï¼ˆPythonï¼‰

```python
# llamaindex_xb_store.py
from typing import List, Optional
import requests
from llama_index.core.vector_stores import VectorStore
from llama_index.core.schema import NodeWithScore, TextNode

class XbVectorStore(VectorStore):
    """xb å‘é‡å­˜å‚¨é€‚é…å™¨"""
    
    def __init__(
        self,
        xb_api_url: str = "http://localhost:8080",
        collection: str = "documents",
    ):
        self.xb_api_url = xb_api_url
        self.collection = collection
    
    def query(
        self,
        query_embedding: List[float],
        top_k: int = 10,
        filters: Optional[dict] = None,
    ) -> List[NodeWithScore]:
        """æŸ¥è¯¢å‘é‡"""
        
        # â­ è°ƒç”¨ xb API
        response = requests.post(
            f"{self.xb_api_url}/api/vector/search",
            json={
                "query": "",
                "embedding": query_embedding,
                "top_k": top_k,
                "filters": filters or {},
            }
        )
        
        data = response.json()
        
        # è½¬æ¢ä¸º LlamaIndex æ ¼å¼
        nodes = []
        for doc in data["documents"]:
            node = TextNode(
                text=doc["content"],
                metadata=doc.get("metadata", {}),
            )
            nodes.append(NodeWithScore(
                node=node,
                score=doc.get("score", 0.0),
            ))
        
        return nodes
```

---

### Step 3: LlamaIndex RAG åº”ç”¨ï¼ˆPythonï¼‰

```python
# rag_app.py
from llama_index.core import VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from llamaindex_xb_store import XbVectorStore

# 1. åˆå§‹åŒ– xb å‘é‡å­˜å‚¨
vector_store = XbVectorStore(
    xb_api_url="http://localhost:8080",
    collection="document_chunks",
)

# 2. åˆ›å»ºç´¢å¼•
embed_model = OpenAIEmbedding()
index = VectorStoreIndex.from_vector_store(
    vector_store=vector_store,
    embed_model=embed_model,
)

# 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“
llm = OpenAI(model="gpt-4")
query_engine = index.as_query_engine(
    llm=llm,
    similarity_top_k=5,
)

# 4. æŸ¥è¯¢
response = query_engine.query("å¦‚ä½•åœ¨ Go ä¸­ä½¿ç”¨ Channelï¼Ÿ")
print(response)
```

---

## ğŸ¨ é«˜çº§åŠŸèƒ½

### 1. æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰

```python
# Python ç«¯
response = requests.post(
    f"{xb_api_url}/api/vector/hybrid_search",
    json={
        "query": "goroutine å¹¶å‘",
        "embedding": embedding,
        "top_k": 10,
        "alpha": 0.5,  # 0.5 = å‘é‡å’Œå…³é”®è¯å„å  50%
    }
)
```

```go
// Go ç«¯
func HybridSearchHandler(c *gin.Context) {
    // 1. å‘é‡æ£€ç´¢ï¼ˆxb + Qdrantï¼‰
    vectorResults := vectorSearch(req.Embedding, req.TopK * 2)
    
    // 2. å…³é”®è¯æ£€ç´¢ï¼ˆPostgreSQL å…¨æ–‡æœç´¢ï¼‰
    keywordResults := keywordSearch(req.Query, req.TopK * 2)
    
    // 3. æ··åˆæ’åºï¼ˆReciprocal Rank Fusionï¼‰
    finalResults := hybridRank(vectorResults, keywordResults, req.Alpha)
    
    c.JSON(200, finalResults[:req.TopK])
}
```

---

### 2. é‡æ’åºï¼ˆRerankingï¼‰

```python
# Python ç«¯ï¼ˆLlamaIndexï¼‰
from llama_index.postprocessor import SentenceTransformerRerank

# ä½¿ç”¨ xb æ£€ç´¢ Top 20
xb_results = vector_store.query(embedding, top_k=20)

# ä½¿ç”¨ Reranker é‡æ’åºåˆ° Top 5
reranker = SentenceTransformerRerank(top_n=5)
final_results = reranker.postprocess_nodes(xb_results)
```

```go
// Go ç«¯ï¼ˆxbï¼‰
built := xb.Of("document_chunks").
    Custom(xb.QdrantBalanced()).
    VectorSearch("embedding", queryVector, 20).  // â­ å…ˆè·å– 20 ä¸ª
    Build()

json, _ := built.JsonOfSelect()
// è¿”å›ç»™ Pythonï¼Œè®© Reranker å¤„ç†
```

---

### 3. å¤šè·³é—®ç­”

```python
# Python ç«¯
from llama_index.core.query_engine import MultiStepQueryEngine

# ç¬¬ä¸€è·³ï¼šä» xb æ£€ç´¢
initial_results = vector_store.query(initial_embedding, top_k=10)

# LLM ç”Ÿæˆç»†åŒ–é—®é¢˜
refined_query = llm.generate_refined_query(initial_results)

# ç¬¬äºŒè·³ï¼šå†æ¬¡ä» xb æ£€ç´¢
final_results = vector_store.query(
    embed(refined_query),
    top_k=5,
    filters={"doc_type": "technical"}
)
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### çº¯ Pythonï¼ˆLlamaIndex + ChromaDBï¼‰

```
æŸ¥è¯¢å»¶è¿Ÿ: ~500ms
- Embedding: 50ms
- å‘é‡æ£€ç´¢: 400ms (Python)
- LLM: 2000ms
```

### Go + Pythonï¼ˆxb + LlamaIndexï¼‰

```
æŸ¥è¯¢å»¶è¿Ÿ: ~100ms
- Embedding: 50ms
- å‘é‡æ£€ç´¢: 20ms (Go + xb) âš¡
- LLM: 2000ms
```

**å‘é‡æ£€ç´¢å¿« 20 å€ï¼** ğŸš€

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. Go ä¸“æ³¨äºæ£€ç´¢ï¼ŒPython ä¸“æ³¨äº AI

```python
# âœ… å¥½çš„åˆ†å·¥
- Go/xb:        å‘é‡æ£€ç´¢ã€æ•°æ®åº“æŸ¥è¯¢ï¼ˆå¿«ï¼‰
- Python/LLM:   Embeddingã€LLM è°ƒç”¨ã€Promptï¼ˆçµæ´»ï¼‰
```

### 2. æ‰¹é‡æ£€ç´¢ä¼˜åŒ–

```python
# âœ… æ‰¹é‡æŸ¥è¯¢
embeddings = [embed(q) for q in questions]

# è°ƒç”¨ xb æ‰¹é‡ API
response = requests.post(
    f"{xb_api_url}/api/vector/batch_search",
    json={
        "embeddings": embeddings,
        "top_k": 5,
    }
)
```

```go
// Go ç«¯ï¼šæ‰¹é‡å¤„ç†
func BatchSearchHandler(c *gin.Context) {
    var req BatchSearchRequest
    c.ShouldBindJSON(&req)
    
    results := make([][]Document, len(req.Embeddings))
    
    for i, embedding := range req.Embeddings {
        built := xb.Of("document_chunks").
            Custom(xb.QdrantBalanced()).
            VectorSearch("embedding", xb.Vector(embedding), req.TopK).
            Build()
        
        json, _ := built.JsonOfSelect()
        results[i] = qdrantClient.Search(json)
    }
    
    c.JSON(200, results)
}
```

---

### 3. ä½¿ç”¨ xb çš„å¤šæ ·æ€§åŠŸèƒ½

```go
// Go ç«¯ï¼šå¤šæ ·æ€§æ£€ç´¢
built := xb.Of("document_chunks").
    Custom(xb.QdrantHighPrecision()).
    VectorSearch("embedding", queryVector, 20).
    WithHashDiversity("semantic_hash").  // â­ è‡ªåŠ¨å»é‡
    Build()

json, _ := built.JsonOfSelect()
// Qdrant è¿”å› 100 ä¸ªï¼Œxb åŸºäºå“ˆå¸Œå»é‡åˆ° 20 ä¸ª
```

---

## ğŸ”§ å®Œæ•´ç¤ºä¾‹é¡¹ç›®

### ç›®å½•ç»“æ„

```
rag-app/
â”œâ”€â”€ README.md
â”œâ”€â”€ LLAMAINDEX_INTEGRATION.md  # æœ¬æ–‡æ¡£
â”œâ”€â”€ go_backend/
â”‚   â”œâ”€â”€ main.go                # Go API æœåŠ¡
â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”œâ”€â”€ search.go          # å‘é‡æ£€ç´¢
â”‚   â”‚   â”œâ”€â”€ hybrid.go          # æ··åˆæ£€ç´¢
â”‚   â”‚   â””â”€â”€ batch.go           # æ‰¹é‡æ£€ç´¢
â”‚   â””â”€â”€ go.mod
â”‚
â””â”€â”€ python_frontend/
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ xb_vector_store.py     # xb é€‚é…å™¨
    â”œâ”€â”€ rag_engine.py          # RAG å¼•æ“
    â””â”€â”€ app.py                 # FastAPI åº”ç”¨
```

---

## ğŸ“– ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

```python
# Python ç«¯
from llama_index.core import SimpleDirectoryReader
from llamaindex_xb_store import XbVectorStore

# 1. åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader('docs/').load_data()

# 2. åˆ†å—å¹¶ä¸Šä¼ åˆ° xb
for doc in documents:
    chunks = chunk_document(doc)
    for chunk in chunks:
        # ä¸Šä¼ åˆ° Go API
        requests.post(
            "http://localhost:8080/api/documents/chunks",
            json={
                "content": chunk.text,
                "embedding": embed(chunk.text),
                "doc_id": doc.id,
                "metadata": chunk.metadata,
            }
        )

# 3. åˆ›å»º RAG ç´¢å¼•
vector_store = XbVectorStore()
index = VectorStoreIndex.from_vector_store(vector_store)

# 4. æŸ¥è¯¢
query_engine = index.as_query_engine()
response = query_engine.query("ä»€ä¹ˆæ˜¯ Goroutineï¼Ÿ")
```

---

### åœºæ™¯ 2: ä»£ç æœç´¢åŠ©æ‰‹

```python
# Python ç«¯ï¼ˆLlamaIndexï¼‰
from llama_index.core.tools import QueryEngineTool

# xb å‘é‡å­˜å‚¨
code_store = XbVectorStore(
    xb_api_url="http://localhost:8080",
    collection="code_vectors"
)

# åˆ›å»ºä»£ç æœç´¢å·¥å…·
code_search_tool = QueryEngineTool(
    query_engine=code_index.as_query_engine(),
    metadata={
        "name": "code_search",
        "description": "æœç´¢ä»£ç åº“ä¸­çš„ç›¸å…³ä»£ç ç‰‡æ®µ"
    }
)

# Agent ä½¿ç”¨å·¥å…·
from llama_index.core.agent import ReActAgent

agent = ReActAgent.from_tools(
    [code_search_tool],
    llm=OpenAI(model="gpt-4"),
)

response = agent.chat("å¦‚ä½•åœ¨ Go ä¸­å®ç°å•ä¾‹æ¨¡å¼ï¼Ÿ")
```

```go
// Go ç«¯ï¼ˆxb APIï¼‰
func CodeSearchHandler(c *gin.Context) {
    built := xb.Of("code_vectors").
        Custom(xb.QdrantHighPrecision()).
        VectorSearch("embedding", queryVector, 20).
        Eq("language", req.Language).
        Gt("quality_score", 0.7).
        WithHashDiversity("semantic_hash").  // ä»£ç å»é‡
        Build()
    
    json, _ := built.JsonOfSelect()
    results := qdrantClient.Search(json)
    
    c.JSON(200, results)
}
```

---

### åœºæ™¯ 3: å¤šæ¨¡æ€æ£€ç´¢

```python
# Python ç«¯
from llama_index.multi_modal import MultiModalVectorStore

# æ–‡æœ¬å‘é‡ â†’ xb Qdrant
text_store = XbVectorStore(collection="text_vectors")

# å›¾åƒå‘é‡ â†’ xb Qdrantï¼ˆä¸åŒ collectionï¼‰
image_store = XbVectorStore(collection="image_vectors")

# å¤šæ¨¡æ€ç´¢å¼•
mm_index = MultiModalVectorStoreIndex.from_vector_stores(
    text_store=text_store,
    image_store=image_store,
)
```

---

## ğŸ”¥ è¿›é˜¶åŠŸèƒ½

### 1. æµå¼å“åº”

```python
# Python ç«¯
def stream_rag_query(question: str):
    # 1. åŒæ­¥æ£€ç´¢ï¼ˆxbï¼Œå¿«é€Ÿï¼‰
    contexts = xb_vector_store.query(embed(question), top_k=5)
    
    # 2. æµå¼ç”Ÿæˆï¼ˆLLMï¼‰
    for chunk in llm.stream_chat(contexts, question):
        yield chunk
```

---

### 2. è‡ªå®šä¹‰ Retriever

```python
from llama_index.core.retrievers import BaseRetriever

class XbHybridRetriever(BaseRetriever):
    """xb æ··åˆæ£€ç´¢å™¨"""
    
    def _retrieve(self, query_bundle):
        # 1. å‘é‡æ£€ç´¢ï¼ˆxb APIï¼‰
        vector_results = requests.post(
            f"{self.xb_api_url}/api/vector/search",
            json={...}
        )
        
        # 2. å…³é”®è¯æ£€ç´¢ï¼ˆxb APIï¼‰
        keyword_results = requests.post(
            f"{self.xb_api_url}/api/keyword/search",
            json={...}
        )
        
        # 3. æ··åˆæ’åº
        return self.hybrid_rank(vector_results, keyword_results)
```

---

### 3. ç¼“å­˜ä¼˜åŒ–

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def xb_search_cached(query_embedding_tuple, top_k):
    """ç¼“å­˜ xb æŸ¥è¯¢ç»“æœ"""
    return xb_vector_store.query(list(query_embedding_tuple), top_k)
```

---

## ğŸ“Š å¯¹æ¯”ï¼šä¸åŒé›†æˆæ–¹æ¡ˆ

| æ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ | æ¨èåº¦ |
|------|------|------|--------|
| **æ–¹æ¡ˆ A: xb HTTP API** | âœ… è§£è€¦<br>âœ… è¯­è¨€æ— å…³<br>âœ… å¯æ‰©å±• | âŒ ç½‘ç»œå¼€é”€ | â­â­â­â­â­ |
| **æ–¹æ¡ˆ B: Go æ’ä»¶** | âœ… æ€§èƒ½é«˜ | âŒ å¤æ‚<br>âŒ Python è°ƒç”¨å›°éš¾ | â­â­ |
| **æ–¹æ¡ˆ C: gRPC** | âœ… æ€§èƒ½é«˜<br>âœ… ç±»å‹å®‰å…¨ | âŒ å¼€å‘æˆæœ¬é«˜ | â­â­â­â­ |
| **æ–¹æ¡ˆ D: çº¯ Python** | âœ… ç®€å• | âŒ æ€§èƒ½å·® | â­â­â­ |

**æ¨èï¼šæ–¹æ¡ˆ Aï¼ˆxb HTTP APIï¼‰** âœ…

---

## ğŸš€ éƒ¨ç½²æ¶æ„

### ç”Ÿäº§ç¯å¢ƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nginx/Traefik   â”‚  è´Ÿè½½å‡è¡¡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”
â”‚Pythonâ”‚  â”‚Pythonâ”‚  â”‚Pythonâ”‚  LlamaIndexï¼ˆAI å¤„ç†ï¼‰
â”‚ App  â”‚  â”‚ App  â”‚  â”‚ App  â”‚
â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜
    â”‚        â”‚        â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
         â”‚        â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚   xb API æœåŠ¡    â”‚  Goï¼ˆé«˜æ€§èƒ½æ£€ç´¢ï¼‰
    â”‚   (å¤šå®ä¾‹)        â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Qdrant    â”‚  å‘é‡æ•°æ®åº“
    â”‚  or Milvus  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

âœ… **æ€§èƒ½**ï¼šGo/xb æ£€ç´¢å¿« 20 å€  
âœ… **çµæ´»**ï¼šPython/LlamaIndex AI ç”Ÿæ€ä¸°å¯Œ  
âœ… **è§£è€¦**ï¼šå„è‡ªç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•  
âœ… **ç±»å‹å®‰å…¨**ï¼šGo ç¼–è¯‘æ—¶æ£€æŸ¥  
âœ… **æ˜“ç”¨**ï¼šxb çš„æµå¼ API + LlamaIndex çš„é«˜å±‚æŠ½è±¡

### å®ç°æˆæœ¬

- **xb API æœåŠ¡**ï¼š~200 è¡Œ Go ä»£ç 
- **LlamaIndex é€‚é…å™¨**ï¼š~100 è¡Œ Python ä»£ç 
- **æ€»è®¡**ï¼š~300 è¡Œä»£ç å®ç°å®Œæ•´é›†æˆ

---

## ğŸ“š ç›¸å…³èµ„æº

- [xb æ–‡æ¡£](https://github.com/fndo-io/xb)
- [LlamaIndex æ–‡æ¡£](https://docs.llamaindex.ai/)
- [xb RAG æœ€ä½³å®è·µ](../../xb/doc/ai_application/RAG_BEST_PRACTICES.md)
- [æ··åˆæ£€ç´¢æŒ‡å—](../../xb/doc/ai_application/HYBRID_SEARCH.md)

---

**å¼€å§‹æ„å»ºä½ çš„é«˜æ€§èƒ½ RAG åº”ç”¨ï¼** ğŸš€

